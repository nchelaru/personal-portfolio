---
title: Data science podcast explorer
author: Nancy Chelaru
date: '2019-11-12'
slug: data-science-podcast-explorer
categories: []
tags: []
description: 'A detailed workflow for creating a live database of data science podcasts'
image: ds_pod.png
keywords: ''
draft: no
---



<p>As a fun programming exercise, I recently created a Shiny app that allows users to explore and discover podcasts on data science, machine learning and big data. This post is a peek behind the curtains, showing how the database that powers the Shiny app is created and set up to update every 24 hours.</p>
<div id="connect-to-postgresql-database" class="section level2">
<h2>1. Connect to PostgreSQL database</h2>
<p>First, we will set up the connection to my <a href="https://data.heroku.com/">Heroku PostgreSQL database</a>, which you can get for free (but limited to 10,000 rows). Sensitive credentials for connecting to the database should be stored somewhere safe, such as an .Renviron file (as I have done here). See <a href="https://db.rstudio.com/best-practices/managing-credentials/">here</a> for a list of options for keeping database credentials safe in R.</p>
<pre class="r"><code>## Import library
library(RPostgres)
 
# Connect to my Heroku PostgreSQL database  
con &lt;- dbConnect(Postgres(), 
                 dbname = &quot;d2s6iokokikv66&quot;, 
                 host = &quot;ec2-174-129-18-42.compute-1.amazonaws.com&quot;, 
                 port = &#39;5432&#39;, 
                 user = &quot;xttqqlhbdystbx&quot;,
                 password = Sys.getenv(&#39;PASSWORD&#39;))</code></pre>
<p><br></p>
</div>
<div id="query-itunes-search-api-for-podcast-listing" class="section level2">
<h2>2. Query iTunes Search API for podcast listing</h2>
<p>Now, we can a listing of podcasts relevant to data science, machine learning, artificial intelligence, and big data from the iTunes store, and do some quick clean-up.</p>
<pre class="r"><code>## Import libraries
library(httr)
library(RCurl)
library(XML)
library(jsonlite)
library(dplyr)
library(anytime)

## Get podcast titles and metadata
url_list &lt;- c(&#39;https://itunes.apple.com/search?term=&quot;data+science&quot;&amp;media=podcast&amp;limit=200&amp;entity=podcast&amp;attribute=descriptionTerm&#39;, 
             &#39;https://itunes.apple.com/search?term=&quot;machine+learning&quot;&amp;media=podcast&amp;limit=200&amp;entity=podcast&amp;attribute=descriptionTerm&#39;, 
             &#39;https://itunes.apple.com/search?term=&quot;artificial+intelligence&quot;&amp;media=podcast&amp;limit=200&amp;entity=podcast&amp;attribute=descriptionTerm&#39;,
             &#39;https://itunes.apple.com/search?term=&quot;big+data&quot;&amp;media=podcast&amp;limit=200&amp;entity=podcast&amp;attribute=descriptionTerm&#39;)

df_list &lt;- list()

for (i in seq_along(url_list)){
  pod &lt;- GET(url_list[i])
  pod_text &lt;- content(pod, &quot;text&quot;)
  pod_json &lt;- fromJSON(pod_text, flatten = TRUE)
  df_list[[i]] &lt;- as.data.frame(pod_json)
}

all_pod &lt;- bind_rows(df_list)

## Deduplicate by podcast title
col_list &lt;- c(&#39;results.collectionName&#39;, 
              &#39;results.feedUrl&#39;, &#39;results.artworkUrl600&#39;, 
              &#39;results.collectionViewUrl&#39;, &#39;results.artistName&#39;, 
              &#39;results.genres&#39;, &#39;results.releaseDate&#39;, 
              &#39;results.trackCount&#39;)

uniques &lt;- all_pod[!duplicated(all_pod[&#39;results.collectionName&#39;]), ][, col_list]

## Split up tags into comma separated strings
uniques$results.genres &lt;- sapply(uniques$results.genres, function(x) paste(x, collapse=&quot;, &quot;)) 

## Filter out podcasts that have not updated since 2018
uniques$results.releaseDate &lt;- anydate(uniques$results.releaseDate)

uniques &lt;- uniques[order(as.Date(uniques$results.releaseDate), decreasing=TRUE),]

uniques &lt;- subset(uniques, format(as.Date(uniques$results.releaseDate),&quot;%Y&quot;) &gt; 2018)

## Remove podcasts without feed URL
uniques &lt;- subset(uniques, !is.na(uniques$results.feedUrl) )

## Remove podcasts without &quot;Technology&quot; tag
uniques &lt;- uniques[grep(&quot;Technology&quot;, uniques$results.genres),]

## Rename columns
colnames(uniques) &lt;- c(&#39;Title&#39;, &#39;FeedURL&#39;, &#39;ArtworkURL&#39;, &#39;URL&#39;, &#39;Creator&#39;, &#39;Tags&#39;, &#39;ReleaseDate&#39;, &#39;TrackCount&#39;)</code></pre>
<p><br></p>
</div>
<div id="scrape-itunes-store-page-for-podcast-description-and-rating" class="section level2">
<h2>3. Scrape iTunes store page for podcast description and rating</h2>
<p>While we had parsed the RSS feed to get the podcast description before, I found that the iTunes store page is much more reliable, as some RSS feed URLs returned by the API were broken. This is also a fun little foray into web scraping using <code>rvest</code>:</p>
<p>Let’s first get the descriptions:</p>
<pre class="r"><code>## Import libraries
library(xml2)
library(rvest)
library(stringr)
library(splitstackshape)

## Define function to scraping for podcast description
get_desc &lt;- function(url){
  tryCatch(
    exp = {
      webpage &lt;- read_html(url)
      
      des &lt;- webpage %&gt;% html_node(&quot;.product-hero-desc__section&quot;) %&gt;% html_text()

      return(str_remove_all(des, &quot;\n&quot;))
        
    },
    error = function(e) {
            return(&#39;Unavailable&#39;)
    },
    finally = {
    }
  )}

## Scrape descriptions
uniques$Description &lt;- sapply(uniques$URL, function(x) get_desc(x))</code></pre>
<p>Then, the ratings:</p>
<pre class="r"><code>## Define function for scraping rating
get_rating &lt;- function(url){
  tryCatch(
    exp = {
      webpage &lt;- read_html(url)
      
      x &lt;- webpage %&gt;% html_node(&quot;.we-star-rating&quot;) %&gt;% html_text()

      return(str_remove_all(x, &quot;\n&quot;))
        
    },
    error = function(e) {
            return(&#39;Unavailable&#39;)
    },
    finally = {
    }
  )
  }

## Scrape ratings  
uniques$Rating &lt;- sapply(uniques$URL, function(x) get_rating(x))</code></pre>
<p><br></p>
</div>
<div id="get-working-feed-urls" class="section level2">
<h2>4. Get working feed URLs</h2>
<p>To be able to show the most recent episodes of a podcast, we need the RSS feed URL. However, some of the podcast RSS feed URLs provided by the iTunes search API seem to be outdated, in that they redirect to a new URL. As <code>getURL()</code> does not follow the redirect, we have to get the redirect URL manually using <code>GET</code> and put that into a new column.</p>
<pre class="r"><code>uniques$NewFeedURL &lt;- sapply(uniques$FeedURL, function(x) GET(x)$url)</code></pre>
<p><br></p>
</div>
<div id="some-final-data-cleaning" class="section level2">
<h2>5. Some final data cleaning</h2>
<p>After getting all the data, we will quickly clean up the data. First, we will split up the <code>Rating</code> column, which contained entries in the format of “5.0, 18 Ratings”, by the comma into the new columns <code>AvgRating</code> and <code>NumRatings</code>. Then, now that we have the description for each podcast, we can filter out ones whose description does not contain “data”, “machine”, “artificial” or “intelligence”. While not perfect, this should get us the podcasts that are likely most relevant to data science.</p>
<pre class="r"><code>## Import library
library(data.table)
library(splitstackshape)

## Split up and rename ratings column
uniques &lt;- cSplit(uniques, &quot;Rating&quot;, sep=&quot;,&quot;)

setnames(uniques, old = c(&#39;Rating_1&#39;, &#39;Rating_2&#39;), new = c(&#39;AvgRating&#39;, &#39;NumRatings&#39;))

## Filter out podcasts that do not contain certain strings in description
uniques &lt;- filter(uniques, grepl(&#39;data|Data|machine|Machine|artificial|Artificial|intelligence|Intelligence&#39;, Description))</code></pre>
<p><br></p>
</div>
<div id="write-data-to-postgresql-database" class="section level2">
<h2>6. Write data to PostgreSQL database</h2>
<p>Finally, we can put store the dataframe in our PostgreSQL database. <code>RPostgres</code> provides very intuitive implementations for writing data to the database:</p>
<pre class="r"><code>dbWriteTable(con, &quot;podcasts&quot;, uniques, overwrite=TRUE)</code></pre>
<p><br></p>
</div>
<div id="pull-data-from-postgresql-database" class="section level2">
<h2>7. Pull data from PostgreSQL database</h2>
<p>We also want to test getting data out of the database, as it will be powering the Shiny app. There are two ways to do this:</p>
<p>First, if we need to use the SQL query results in a R script, we can go for the <code>RPostgres</code> implementation:</p>
<pre class="r"><code>df &lt;- dbGetQuery(con, &quot;SELECT * FROM podcasts&quot;)</code></pre>
<p><br></p>
<p>Or, you can opt for the SQL code chunk if working in a R notebook or Rmarkdown document, in which you can write native SQL queries. To store the output in a R variable that then can be used in a R code chunk, just specify the variable name in the header, for example:</p>
<pre class="plain"><code>```{sql, connection=con, output.var = &quot;df&quot;}
SELECT * FROM podcasts 
```
</code></pre>
<p><br></p>
</div>
<div id="create-a-continuously-updating-database" class="section level2">
<h2>8. Create a continuously updating database</h2>
<p>Finally, having putting all of this into an R script, I am now using a free-tier Heroku Scheduler to run it every 24 hours to ensure that all the information in the database is up-to-date. The set up for a R script is a bit more complicated, but I have followed this <a href="https://www.r-bloggers.com/running-an-r-script-on-heroku/">nice tutorial</a> on <b>Roel’s R-tefacts</b> without issue each time.</p>
<p>Phew, that was a long post! Now that we are done with the set up, check ou the working product at <a href="https://nancy-chelaru-centea.shinyapps.io/ds_podcast_db/" class="uri">https://nancy-chelaru-centea.shinyapps.io/ds_podcast_db/</a>! :)</p>
</div>
